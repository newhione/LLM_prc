{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNN\n",
        "```\n",
        "- 데이터 -> 토큰화&정수 인코딩 -> 시퀀스 패딩\n",
        "- baseline 일반신경망 Dense\n",
        "- Simple RNN 기울기소실 문제\n",
        "- Bidirectional LSTM: 양방향 학습\n",
        "```\n",
        "\n",
        "> - Tokenization <br>\n",
        ">   - 텍스트를 숫자로 변환 <br>\n",
        "> - Word Embedding <br>\n",
        ">   - 단어를 고차원 벡터로 변환 <br>\n",
        "> - Sequence Padding\n",
        ">   - 길이가 다른 문장을 같은 크기로 고정\n",
        "\n",
        "> - LSTM\n",
        ">   - RNN의 경사소실 문제 해결\n",
        "> - Bidirectional LSTM\n",
        ">   - 양방향 컨텐츠 학습\n",
        "\n",
        "```\n",
        "토큰화: 숫자로 변환하는 과정\n",
        "<토크나이저 3단계>\n",
        "1. fit_on_text(texts) 가장 빈도가 높은 단어의 인덱스를 구축 -> 딕셔너리\n",
        "2. texts_to_sequence(texts) 각문서를 정수 시퀀스로 변환\n",
        "3. pad_sequence() 길이 정규화(같은 길이로)\n",
        "```\n",
        "\n",
        "딥러닝에서 Word Embedding : 각 단어를 고정된 크기의 실수 벡터로"
      ],
      "metadata": {
        "id": "S1egjCb_W_NL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t884r6S1VlRr"
      },
      "outputs": [],
      "source": [
        "sample_reviews = [\n",
        "    \"this movie is great and wonderful\",\n",
        "    \"bad movie with poor acting\",\n",
        "    \"great movie absolutely wonderful\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이토치 버전\n",
        "# 토큰화\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "p6TTE8Y2VsRV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 단어 분할 및 빈도"
      ],
      "metadata": {
        "id": "DQYS9AjZdTlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 단어 분할 및 빈도 계산\n",
        "all_words = []\n",
        "for i in [review.split() for review in sample_reviews]:\n",
        "    all_words.extend(i)\n",
        "\n",
        "# 단어빈도\n",
        "word_freq = Counter(all_words)\n",
        "word_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27OrfMldVsTv",
        "outputId": "76279e05-c527-4f22-b40d-64e2c59859ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'this': 1,\n",
              "         'movie': 3,\n",
              "         'is': 1,\n",
              "         'great': 2,\n",
              "         'and': 1,\n",
              "         'wonderful': 2,\n",
              "         'bad': 1,\n",
              "         'with': 1,\n",
              "         'poor': 1,\n",
              "         'acting': 1,\n",
              "         'absolutely': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tokenizer"
      ],
      "metadata": {
        "id": "lntzJjuIdZAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Tokenizer 구현\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, num_words=10, oov_token = 'UNK'):\n",
        "        self.num_words = num_words\n",
        "        self.oov_token = oov_token\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "    def fit_on_texts(self, texts):\n",
        "        '''문장을 단어 인덱스로 변환 '''\n",
        "        all_words = []\n",
        "        for i in [review.split() for review in sample_reviews]:\n",
        "            all_words.extend(i)\n",
        "        word_freq = Counter(all_words)\n",
        "        # 빈도 높은 순서로 인덱스 부여\n",
        "        # oov 토큰을 1로 설정\n",
        "        self.word_index[self.oov_token] =1\n",
        "        self.index_word[1]  = self.oov_token\n",
        "        idx = 2\n",
        "        for word, _ in word_freq.most_common(self.num_words-1):\n",
        "            self.word_index[word] = idx\n",
        "            self.index_word[idx] = word\n",
        "            idx += 1\n",
        "    def texts_to_sequences(self, texts):\n",
        "        '''텍스트를 정수 시퀀스로 변환'''\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            seq = []\n",
        "            for word in text.split():\n",
        "                # 단어가 vocabulary에 있으면 인덱스를 사용 없으면 oov\n",
        "                word_index = self.word_index.get(word, 1)\n",
        "                seq.append(word_index)\n",
        "            sequences.append(seq)\n",
        "        return sequences\n",
        "# Tokenizer 생성 및 학습\n",
        "tokenizer =  SimpleTokenizer(num_words=10, oov_token='UNK')\n",
        "tokenizer.fit_on_texts(sample_reviews)\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYQMlZVYVsV5",
        "outputId": "8bf5367a-9370-47fc-fff2-2b3dca9902c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'UNK': 1,\n",
              " 'movie': 2,\n",
              " 'great': 3,\n",
              " 'wonderful': 4,\n",
              " 'this': 5,\n",
              " 'is': 6,\n",
              " 'and': 7,\n",
              " 'bad': 8,\n",
              " 'with': 9,\n",
              " 'poor': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트를 시퀀스로 변환\n",
        "sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbJSC4UsVsYJ",
        "outputId": "7219eca2-6cf3-4a6f-cbd3-4379a790ba8a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 2, 6, 3, 7, 4], [8, 2, 9, 10, 1], [3, 2, 1, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1sJr8vjVsbT",
        "outputId": "7c2677da-22d5-464f-c6a8-da7be9507dc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this movie is great and wonderful',\n",
              " 'bad movie with poor acting',\n",
              " 'great movie absolutely wonderful']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Padding"
      ],
      "metadata": {
        "id": "Lg3fnYnYdcFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 구현 - 문자열의 길이를 동일하게 맞춘다\n",
        "def pad_sequence_manual(sequences, max_len=10, padding='pre',value=0):\n",
        "    '''패딩구현'''\n",
        "    padded = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) >= max_len:\n",
        "            if padded == 'pre':\n",
        "                padded_seq = seq[-max_len:]\n",
        "            else:\n",
        "                padded_seq = seq[:max_len]\n",
        "        else:\n",
        "            pad_length = max_len-len(seq)\n",
        "            if padding == 'pre':\n",
        "                padded_seq = [value]*pad_length + seq\n",
        "            else:\n",
        "                padded_seq = seq + [value]*pad_length\n",
        "        padded.append(padded_seq)\n",
        "    return np.array(padded)\n",
        "padded = pad_sequence_manual(sequences)\n",
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44C0YgUyWvr7",
        "outputId": "3a941925-6d71-42de-c494-3a11cb919635"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  5,  2,  6,  3,  7,  4],\n",
              "       [ 0,  0,  0,  0,  0,  8,  2,  9, 10,  1],\n",
              "       [ 0,  0,  0,  0,  0,  0,  3,  2,  1,  4]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch tensor 변환\n",
        "sequence_tensor =  torch.LongTensor(padded)\n",
        "sequence_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7emS0VrhWyev",
        "outputId": "fa10a61f-593f-4853-d37f-4a668ed17651"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0,  5,  2,  6,  3,  7,  4],\n",
              "        [ 0,  0,  0,  0,  0,  8,  2,  9, 10,  1],\n",
              "        [ 0,  0,  0,  0,  0,  0,  3,  2,  1,  4]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 워드 임베딩"
      ],
      "metadata": {
        "id": "bvtFnVZbdjlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 워드 임베딩 - 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "print(f'패딩된 시퀀스 형태: {sequence_tensor.shape}')\n",
        "print(f'첫번째: {sequence_tensor[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "424UWq2hV1Fz",
        "outputId": "79d07634-58e9-42e6-e677-6f7c1ac82f3d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "패딩된 시퀀스 형태: torch.Size([3, 10])\n",
            "첫번째: tensor([0, 0, 0, 0, 5, 2, 6, 3, 7, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# pytorch embedding: 레이어 생성\n",
        "# num_embeddings: 어휘의 크기\n",
        "# embedding_dim: 각 단어를 몇 차원 벡터로 표현할 것인지\n",
        "# padding_idx: 길이 맞출 때 채우는 값\n",
        "\n",
        "embedding_layer = nn.Embedding(num_embeddings=1000, embedding_dim=8, padding_idx=0)\n",
        "embedded = embedding_layer(sequence_tensor)\n",
        "\n",
        "print(f'입력형태: {sequence_tensor.shape}')\n",
        "print(f'출력형태: {embedded.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cET0M6DUWzEo",
        "outputId": "27890ff3-dd3d-44f5-e12d-c600af822dd4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력형태: torch.Size([3, 10])\n",
            "출력형태: torch.Size([3, 10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 벡터 상세 분석\n",
        "# sample data의 첫 3개 단어 임베딩\n",
        "\n",
        "for word_idx in range(3):\n",
        "  embedding_vec = embedded[0,word_idx].detach().numpy()\n",
        "  word_id = sequence_tensor[0,word_idx].item()\n",
        "  print(f'단어 id {word_id} : {embedding_vec}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Aep-Sf0YJcF",
        "outputId": "1c3f8a57-345e-4f3a-9aa6-599a4c05970c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 id 0 : [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "단어 id 0 : [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "단어 id 0 : [0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 행렬\n",
        "embedding_matrix = embedding_layer.weight.detach().numpy()\n",
        "\n",
        "print(f'임베딩 행렬 형태: {embedding_matrix.shape}') #1000,8\n",
        "print(f'패딩(id=0)의 임베딩: {embedding_matrix[0]}')\n",
        "print(f'단어(id=5)의 임베딩: {embedding_matrix[5]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5hM1wnuYJe3",
        "outputId": "db5a139c-a62e-48c2-895a-480b7df7364d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 행렬 형태: (1000, 8)\n",
            "패딩(id=0)의 임베딩: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "단어(id=5)의 임베딩: [ 0.62010115  0.93770546 -2.8720152   2.1677155  -0.77353656  0.5262479\n",
            " -2.0368896  -1.463761  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## RNN 적용"
      ],
      "metadata": {
        "id": "JPKLcvGZahY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RnnModule(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(RnnModule, self).__init__()\n",
        "        self.embdding = nn.Embedding(vocab_size,embedding_dim,padding_idx=0)\n",
        "        self.rnn  = nn.RNN(embedding_dim,hidden_dim,batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_emb =  self.embdding(x)  #(batch,seq_len, embedding_dim)\n",
        "        rnn_out, h_n = self.rnn(x_emb) # rnn_out(batch,seq_len,hidden_dim)\n",
        "                                     # h_n  (1, batch,hidden_dim)\n",
        "        # 마지막 스텝의 출력\n",
        "        last_output = rnn_out[:,-1,:]   # (batch,hidden_dim)\n",
        "        output = self.sigmoid(self.fc(last_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "cBXUWwlNae5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oa_8tSI5ae77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xm7jRR3aae_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}