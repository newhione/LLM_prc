{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "bWemP-BkP513"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ÏûêÏó∞Ïñ¥ Í∞êÏÑ±Î∂ÑÏÑù\n",
        "- Í∞êÏÑ±ÏÇ¨Ï†Ñ Í∏∞Î∞ò: ÎØ∏Î¶¨Ï†ïÏùòÎêú Í∞êÏÑ± Îã®Ïñ¥ ÏÇ¨Ï†Ñ ÏÇ¨Ïö©(Í∑úÏπô ÏÇ¨Ïö©)\n",
        "  - TextBlob, AFINNm VADER\n",
        "- Î®∏Ïã†Îü¨Îãù Í∏∞Î∞ò: Îç∞Ïù¥ÌÑ∞Î°úÎ∂ÄÌÑ∞ Ìå®ÌÑ¥ ÌïôÏäµ(ÌÜµÍ≥Ñ Í∏∞Î∞ò)\n",
        "  - TF-IDF Î≤°ÌÑ∞Ìôî\n",
        "  - ÏÑ†ÌòïÌöåÍ∑Ä\n",
        "  - Î°úÏßÄÏä§Ìã±ÌöåÍ∑Ä\n",
        "  - F1 score, Recision, Recall -> classification report\n",
        "- ÏÇ¨Ïö©Îç∞Ïù¥ÌÑ∞:\n",
        "  - NLTK ÏòÅÌôî Î¶¨Î∑∞(2000Í∞ú)\n",
        "  - Îã§Ïùå ÏòÅÌôîÎ¶¨Î∑∞\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Vmo_QGGKO8_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Í∞êÏ†ïÎ∂ÑÏÑù: Í∞êÏÑ±ÏÇ¨Ï†Ñ Í∏∞Î∞ò\n",
        "\n",
        "## ÏïåÍ≥†Î¶¨Ï¶ò\n",
        "- TextBlob: ÏÇ¨Ï†ÑÍ∏∞Î∞ò Í∞êÏÑ±Î∂ÑÏÑù\n",
        "- AFINN: Í∞êÏ†ï Ï†êÏàò Îß§Ìïë\n",
        "- VADER(Valance Aware Dictionary): ÏÜåÏÖúÎØ∏ÎîîÏñ¥ ÏµúÏ†ÅÌôî Í∞êÏÑ±Î∂ÑÏÑù\n",
        "- TF-IDF: ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞Ìôî\n",
        "- Multinomial Navie Bayes: ÌôïÎ•† Í∏∞Î∞ò Î∂ÑÎ•ò\n",
        "- Liner Regression\n",
        "- Logistic Regression"
      ],
      "metadata": {
        "id": "bWemP-BkP513"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. TextBlob\n",
        "```\n",
        "Ïù¥ ÏòÅÌôîÎäî Ï†ïÎßê Ï¢ãÍ≥† Ïû¨ÎØ∏ÏûàÎã§\n",
        "- Ï¢ãÎã§ +1(Í∏çÏ†ï)\n",
        "- Ïû¨ÎØ∏ÏûàÎã§ +1(Í∏çÏ†ï)\n",
        "- +2 > 0 --> Í∏çÏ†ï(pos) Î∂ÑÎ•ò\n",
        "```\n",
        "**Polarity(Í∑πÏÑ±ÎèÑ)**\n",
        "- (Í∏çÏ†ïÎã®Ïñ¥ Í∞úÏàò - Î∂ÄÏ†ïÎã®Ïñ¥ Í∞úÏàò) / Ï†ÑÏ≤¥Îã®Ïñ¥ Í∞úÏàò\n",
        "- Î≤îÏúÑ: -1.0~+1.0  (0:Ï§ëÎ¶Ω)\n",
        "\n",
        "**Subjectivity(Ï£ºÍ¥ÄÏÑ±)**\n",
        "- ÌèâÍ∞ÄÎåÄÏÉÅ Îã®Ïñ¥Ïùò ÎπÑÏú®\n",
        "- Î≤îÏúÑ: 0.0~1.0 (0:Í∞ùÍ¥ÄÏ†Å, 1:Ï£ºÍ¥ÄÏ†Å)\n",
        "\n",
        "Î¨∏Îß•ÏùÑ Î¨¥ÏãúÌïòÍ≥† Îã®Ïñ¥Ïùò Í∑πÏÑ±Îßå Í≥†Î†§Ìï®.\n",
        "```\n",
        "Ïù¥ ÏòÅÌôîÎäî ÎÇòÏÅòÏßÄ ÏïäÎã§. -> ÎÇòÏÅòÎã§(-) *ÏïäÎã§(-)*Î°ú Ïù∏ÏãùÌï®\n",
        "Ïû•Ï†ê: Îπ†Î•∏ ÏÜçÎèÑ, ÌïôÏäµÏù¥ Î∂àÌïÑÏöî\n",
        "ÏÇ¨Ïö©: Ïã§ÏãúÍ∞Ñ Í∞êÏÑ±Î∂ÑÏÑù, Ïä§Ìä∏Î¶¨Î∞ç Îç∞Ïù¥ÌÑ∞\n",
        "```\n"
      ],
      "metadata": {
        "id": "VJaQVqj_QXVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sfXYoefRspb",
        "outputId": "a2a73286-6988-4f85-84c5-1b2a0cf1264e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWXqTMOfO3My",
        "outputId": "ae7b58ec-6c67-4e6d-e5eb-787aa625b798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sentence(\"TextBlob is amazingly simple to use.\"), Sentence(\"What a wonderful library for NLP!\")]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "['TextBlob', 'is', 'amazingly', 'simple', 'to', 'use', 'What', 'a', 'wonderful', 'library', 'for', 'NLP']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[('TextBlob', 'NNP'), ('is', 'VBZ'), ('amazingly', 'RB'), ('simple', 'JJ'), ('to', 'TO'), ('use', 'VB'), ('What', 'WP'), ('a', 'DT'), ('wonderful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('NLP', 'NNP')]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "['textblob', 'wonderful library', 'nlp']\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob, Word\n",
        "\n",
        "text = \"TextBlob is amazingly simple to use. What a wonderful library for NLP!\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "print(blob.sentences)\n",
        "print('-'*200)\n",
        "print(blob.words)\n",
        "print('-'*200)\n",
        "print(blob.tags)\n",
        "print('-'*200)\n",
        "print(blob.noun_phrases)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Í∞êÏÑ±Î∂ÑÏÑù\n",
        "print(blob.sentiment)\n",
        "print('-'*100)\n",
        "print(f'Polarity: {blob.sentiment.polarity}')\n",
        "print(f'Subjectivity: {blob.sentiment.subjectivity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-VMmxJtTNXV",
        "outputId": "aae47c05-f0b0-4d66-aada-a14d8f143530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.5, subjectivity=0.6785714285714286)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Polarity: 0.5\n",
            "Subjectivity: 0.6785714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. AFINN\n",
        "- Lexicon-Based\n",
        "- Í∞Å Îã®Ïñ¥Ïùò `-5~+5`Ïùò Ï†êÏàòÎ•º Î∂ÄÏó¨ÌïòÍ≥† Ìï©ÏÇ∞\n",
        "```\n",
        "Ïù¥ ÏòÅÌôîÎäî Ï¢ãÏßÄÎßå, Ï¢ãÏßÄ ÏïäÏùÄ Î∂ÄÎ∂ÑÎèÑ ÏûàÎã§.\n",
        "- Ï¢ãÎã§+3 Ï¢ãÎã§+3 ÎÇòÏÅòÎã§-3  =>  +3>0 (Í∏çÏ†ï)\n",
        "```\n",
        "- `score = sum(word_sentiment_value)`\n",
        "- Î∂ÑÎ•òÍ∑úÏπô: `score>0`(Í∏çÏ†ï), `score<0`(Î∂ÄÏ†ï)\n",
        "- Ïù¥Î™®Ìã∞ÏΩò ÏßÄÏõê\n",
        "- Í∞ïÎèÑÌëúÌòÑ Ïù∏Ïãù `very, really..`\n",
        "- Í∞ïÏ°∞ ÏàòÏ†ïÏûê(Intensifiers)\n",
        "  - Îß§Ïö∞ Ï¢ãÎã§ = 1.5*(Ï¢ãÎã§Ïùò Ï†êÏàò)\n",
        "\n",
        "---\n",
        "\n",
        "TextBlob vs. AFINN\n",
        "- TextBlob: Îçî ÏùºÎ∞òÏ†ÅÏù∏ Ï†ëÍ∑º\n",
        "- AFINN: Îçî Ï†ïÌôïÌïú Ï†êÏàò Îß§Ìïë"
      ],
      "metadata": {
        "id": "K08Nf1kuUA-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from afinn import Afinn\n",
        "\n",
        "af = Afinn()\n",
        "\n",
        "text1 = \"TextBlob is amazingly simple to use.\"\n",
        "text2 = \"What a wonderful library for NLP!\"\n",
        "\n",
        "score1 = af.score(text1)\n",
        "score2 = af.score(text2)\n",
        "\n",
        "score1, score2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjPRnsIhTNaM",
        "outputId": "626ec880-6f19-4feb-a5d4-e6beb4fb0925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 4.0)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. VADER\n",
        "- rule-based\n",
        "- **ÏÜåÏÖúÎØ∏ÎîîÏñ¥ ÌÖçÏä§Ìä∏**Ïóê ÏµúÏ†ÅÌôî\n",
        "```\n",
        "Ïù¥ ÏòÅÌôîÎäî Ï†ïÎßêÏ†ïÎßê ÌõåÎ•≠Ìï¥!!!\n",
        "- ÌõåÎ£ΩÌïòÎã§(Í∏∞Î≥∏) +0.7\n",
        "- Ï†ïÎßêÏ†ïÎßê(Í∞ïÏ°∞) x1.5\n",
        "- !!!(Î¨∏Ïû•Î∂ÄÌò∏ Í∞ïÏ°∞) x1.2\n",
        "```\n",
        "- **4Í∞úÏùò Í∞êÏ†ïÏßÄÏàò**\n",
        "  - positive (Í∏çÏ†ïÌôïÎ•†) `0~1`\n",
        "  - negative (Î∂ÄÏ†ïÌôïÎ•†) `0~1`\n",
        "  - neutral (Ï§ëÎ¶ΩÌôïÎ•†) `0~1`\n",
        "  - compound (Ï¢ÖÌï©Ï†êÏàò) `-1~1`\n",
        "\n",
        "- **Score**\n",
        "```\n",
        "score = compound_score / sqrt(compound_score**2 + 0.0625)\n",
        "- score >= 0.05 (Í∏çÏ†ï)\n",
        "- score <= -0.05 (Î∂ÄÏ†ï)\n",
        "- -0.05 < score < 0.05  (Ï§ëÎ¶Ω)\n",
        "```\n",
        "\n",
        "- **ÎåÄÏÜåÎ¨∏Ïûê Íµ¨Î∂Ñ**\n",
        "```\n",
        "AMAZING amazing Îã§Î•∏ Ï†êÏàò\n",
        "```\n",
        "- **Ïù¥Î™®Ìã∞ÏΩò ÏßÄÏõê** -> ÏÜåÏÖúÎØ∏ÎîîÏñ¥ text ÏµúÏ†ÅÌôî\n",
        "```\n",
        ":) Í∏çÏ†ï :-( Î∂ÄÏ†ï\n",
        "```\n"
      ],
      "metadata": {
        "id": "lQPeAKpqWXT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONzPWxELTNdx",
        "outputId": "5fa7d28a-fbb6-45f2-b459-dfc96876c1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "sentences = [\n",
        "    \"I love this product! It's absolutely amazing üòç\",\n",
        "    \"This is the worst movie I've ever seen...\",\n",
        "    \"The food was okay, not great but not bad either.\",\n",
        "    \"I‚Äôm REALLY happy with the results!!!\",\n",
        "    \"Not good at all. I‚Äôm disappointed.\",\n",
        "]"
      ],
      "metadata": {
        "id": "YRnDok1DSpxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in sentences:\n",
        "  scores = analyzer.polarity_scores(s)\n",
        "  print(f'Sentence -> {s}')\n",
        "  print(f'Score -> {scores}')\n",
        "  print('-'*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dsamCoBO7ao",
        "outputId": "50659f38-f6a0-48fa-9598-f7af1cd8c387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence -> I love this product! It's absolutely amazing üòç\n",
            "Score -> {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.862}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sentence -> This is the worst movie I've ever seen...\n",
            "Score -> {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sentence -> The food was okay, not great but not bad either.\n",
            "Score -> {'neg': 0.149, 'neu': 0.487, 'pos': 0.364, 'compound': 0.4728}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sentence -> I‚Äôm REALLY happy with the results!!!\n",
            "Score -> {'neg': 0.0, 'neu': 0.472, 'pos': 0.528, 'compound': 0.7651}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sentence -> Not good at all. I‚Äôm disappointed.\n",
            "Score -> {'neg': 0.579, 'neu': 0.421, 'pos': 0.0, 'compound': -0.6711}\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from textblob import TextBlob\n",
        "from afinn import Afinn\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# nltk Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú\n",
        "nltk.download('movie_reviews', quiet=True)\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "\n",
        "# ÏòÅÌôîÎ¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "fileids = movie_reviews.fileids()"
      ],
      "metadata": {
        "id": "Oj-8WxTbdhs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [movie_reviews.raw(fileid) for fileid in fileids[:50]] + [movie_reviews.raw(fileid) for fileid in fileids[-50:]]\n",
        "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids[:50]] + [movie_reviews.categories(fileid)[0] for fileid in fileids[-50:]]\n",
        "\n",
        "len(reviews), categories.count('pos'), categories.count('neg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr0bwQ0Zdhwe",
        "outputId": "b12f109d-0cac-416f-903d-ad07fbf6de7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 50, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# TextBlob\n",
        "def sentiment_textblob(docs):\n",
        "  return ['pos' if TextBlob(doc).sentiment.polarity > 0 else 'neg' for doc in docs]\n",
        "\n",
        "predictions_textblob = sentiment_textblob(reviews)\n",
        "accuracy_textblob = accuracy_score(categories, predictions_textblob)\n",
        "print(f'TextBlob Accuracy: {accuracy_textblob}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRDNN8V2O7d4",
        "outputId": "11ed5cbf-04e8-4fd9-95d1-a4a18c1c2358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextBlob Accuracy: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AFINN\n",
        "def sentiment_afinn(docs):\n",
        "  afn = Afinn()\n",
        "  return ['pos' if afn.score(doc) > 0 else 'neg' for doc in docs]\n",
        "\n",
        "prediction_afinn = sentiment_afinn(reviews)\n",
        "accuracy_afinn = accuracy_score(categories, prediction_afinn)\n",
        "print(f'AFINN Accuracy: {accuracy_afinn}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bWt5cgoeRkA",
        "outputId": "d67c6eb5-dd53-4809-93a2-01f06e683cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AFINN Accuracy: 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER\n",
        "def sentiment_vader(docs):\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  return ['pos' if analyzer.polarity_scores(doc)['compound'] > 0 else 'neg' for doc in docs]\n",
        "\n",
        "prediction_vader = sentiment_vader(reviews)\n",
        "accuracy_vader = accuracy_score(categories, prediction_vader)\n",
        "print(f'VADER Accuracy: {accuracy_vader}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ouf_LjayeRnS",
        "outputId": "2a8e6ba3-a5eb-4852-ace9-a9ff1806afed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VADER Accuracy: 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Í∞êÏÑ±Î∂ÑÏÑù: Î®∏Ïã†Îü¨Îãù Í∏∞Î∞ò\n",
        "\n",
        "- ÎÇòÏù¥Î∏å Î≤†Ïù¥Ï¶à\n",
        "  - Î≤†Ïù¥Ï¶à Ï†ïÎ¶¨Ïóê Í∏∞Î∞òÌïú ÌôïÎ•†Ï†Å Î∂ÑÎ•ò Î™®Îç∏\n",
        "  ```\n",
        "  \"Ï¢ãÎã§\"Îã®Ïñ¥Î•º Î≥∏ ÌõÑ Ïù¥ Î¶¨Î∑∞Í∞Ä Í∏çÏ†ïÏùº ÌôïÎ•†ÏùÄ?\n",
        "  - p(Í∏çÏ†ï|\"Ï¢ãÎã§\") = (p(\"Ï¢ãÎã§\"|Í∏çÏ†ï) x p(Í∏çÏ†ï)) / p(\"Ï¢ãÎã§\")\n",
        "  ```\n",
        "- **Î¨∏ÏÑú**ÎÇò **Î¶¨Î∑∞** Í∞ôÏùÄ ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú, <br>\n",
        "**\"Ïù¥ Î¨∏Ïû•Ïù¥ Í∏çÏ†ïÏùº ÌôïÎ•† vs Î∂ÄÏ†ïÏùº ÌôïÎ•† Ï§ë Ïñ¥Îäê Ï™ΩÏù¥ Îçî ÌÅ∞Í∞Ä?\"** <br>\n",
        "Î•º Í≥ÑÏÇ∞Ìï¥ Í∞êÏ†ïÏùÑ Î∂ÑÎ•ò"
      ],
      "metadata": {
        "id": "jGgJOnOQjV1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "ABnHmztteRqs"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Îç∞Ïù¥ÌÑ∞ Î∂Ñ\n",
        "dataset = train_test_split(reviews, categories, test_size=0.2, random_state=42, stratify=categories)\n",
        "\n",
        "len(dataset[0]), len(dataset[2])"
      ],
      "metadata": {
        "id": "zBQWPNIVjcEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab79ad5-9f63-4dbe-d7fc-3d547e73b20d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 80)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf Î≤°ÌÑ∞Ìôî\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "X_train = vectorizer.fit_transform(dataset[0])\n",
        "X_test = vectorizer.transform(dataset[1])\n",
        "y_train = dataset[2]\n",
        "y_test = dataset[3]"
      ],
      "metadata": {
        "id": "-7QIRYgSjcGq"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. mnb\n",
        "mnb_clf = MultinomialNB()\n",
        "mnb_clf.fit(X_train, y_train)\n",
        "mnb_pred = mnb_clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, mnb_pred))"
      ],
      "metadata": {
        "id": "iSJSlGyYjcKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b469f97-2200-45b9-9cec-03771b209ad9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.54      0.70      0.61        10\n",
            "         pos       0.57      0.40      0.47        10\n",
            "\n",
            "    accuracy                           0.55        20\n",
            "   macro avg       0.55      0.55      0.54        20\n",
            "weighted avg       0.55      0.55      0.54        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr_pred = lr.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, lr_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uVs-igwpmyB",
        "outputId": "349096f4-d4b3-46a8-e9e7-5f5ca9e42273"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.62      0.80      0.70        10\n",
            "         pos       0.71      0.50      0.59        10\n",
            "\n",
            "    accuracy                           0.65        20\n",
            "   macro avg       0.66      0.65      0.64        20\n",
            "weighted avg       0.66      0.65      0.64        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. ÏÑ±Îä•Ìñ•ÏÉÅ\n",
        "- ÏÜåÎ¨∏Ïûê Î≥ÄÌôò\n",
        "- Ïó∞ÏÜçÎêú Î¨∏ÏûêÏó¥ Ï§ë, 3Í∏ÄÏûê Ïù¥ÏÉÅ\n",
        "- Ïñ¥Í∞ÑÏ∂îÏ∂ú(ÌòïÌÉúÏÜå Î∂ÑÏÑù)\n",
        "- Î∂àÏö©Ïñ¥ Ï†úÍ±∞"
      ],
      "metadata": {
        "id": "5_Zam4NssqbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "zyUVMBrfpm0S"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWyatPLWxK0F",
        "outputId": "ed14eef5-20dc-45b6-e02c-475b98350947"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_tokenizer(text):\n",
        "    text = text.lower()\n",
        "    tokenizer =  RegexpTokenizer(r\"[\\w']{3,}\")\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    porter = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [porter.stem(token) for token in tokens if token not in stop_words]\n",
        "vector = TfidfVectorizer(\n",
        "    tokenizer  = custom_tokenizer\n",
        "    ,max_features=1000\n",
        "    ,min_df=5\n",
        "    ,max_df=0.5\n",
        "    ,token_pattern = r\"[\\w']{3,}\"\n",
        ")\n",
        "x_train = vector.fit_transform(dataset[0])\n",
        "x_test = vector.transform(dataset[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgTy_4l_pm3r",
        "outputId": "ca1dfbb3-9718-43a2-a0a6-3dc2706b637e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model):\n",
        "    model.fit(x_train,y_train)\n",
        "    predict = model.predict(x_test)\n",
        "    print( classification_report(y_test, predict))"
      ],
      "metadata": {
        "id": "JdMZgb7FtH4W"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(LogisticRegression())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESsFHjiotH6V",
        "outputId": "2354e5fc-7638-4733-ecb4-058f19d29fcd"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.80      0.80      0.80        10\n",
            "         pos       0.80      0.80      0.80        10\n",
            "\n",
            "    accuracy                           0.80        20\n",
            "   macro avg       0.80      0.80      0.80        20\n",
            "weighted avg       0.80      0.80      0.80        20\n",
            "\n"
          ]
        }
      ]
    }
  ]
}